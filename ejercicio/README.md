# ğŸ¤– Chatbot RAG - TechInnovate Solutions

Sistema de chatbot conversacional que utiliza la tÃ©cnica RAG (Retrieval-Augmented Generation) con LangChain y OpenAI para responder preguntas basÃ¡ndose en documentos markdown de la empresa TechInnovate Solutions.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un chatbot inteligente que puede responder preguntas sobre una empresa ficticia utilizando informaciÃ³n almacenada en documentos markdown. El sistema utiliza embeddings para vectorizar los documentos y un vector store en memoria para recuperar informaciÃ³n relevante que luego es utilizada por un modelo de lenguaje (LLM) para generar respuestas contextualizadas.

## ğŸ—ï¸ Arquitectura

El proyecto sigue una arquitectura modular:

```
ejercicio/
â”œâ”€â”€ main.py              # Punto de entrada de la aplicaciÃ³n
â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ documento1.md    # InformaciÃ³n general de la empresa
â”‚   â””â”€â”€ documento2.md    # PolÃ­ticas y procedimientos internos
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_system.py    # Sistema RAG principal
â”‚   â””â”€â”€ chatbot.py       # LÃ³gica del chatbot
â”œâ”€â”€ requirements.txt     # Dependencias del proyecto
â”œâ”€â”€ .env                 # Variables de entorno (API keys)
â””â”€â”€ README.md            # Este archivo
```

## ğŸš€ CaracterÃ­sticas

- âœ… Sistema de embeddings usando `text-embedding-3-small` de OpenAI
- âœ… Vector store en memoria con `InMemoryVectorStore` de LangChain
- âœ… Chatbot conversacional con modelos GPT-4o/GPT-4.1/GPT-4o-mini
- âœ… TÃ©cnica RAG para recuperaciÃ³n de informaciÃ³n relevante
- âœ… Procesamiento y vectorizaciÃ³n de documentos markdown
- âœ… Mantenimiento del contexto conversacional
- âœ… Respuestas basadas Ãºnicamente en documentos procesados
- âœ… Sistema de retrieval con bÃºsqueda por similitud
- âœ… Interfaz de lÃ­nea de comandos (CLI) interactiva

## ğŸ“¦ Requisitos Previos

- Python 3.8 o superior
- Cuenta de OpenAI con API key
- pip (gestor de paquetes de Python)

## ğŸ”§ InstalaciÃ³n

1. **Clonar o descargar el proyecto**

2. **Instalar las dependencias**

```bash
pip install -r requirements.txt
```

3. **Configurar las variables de entorno**

AsegÃºrate de que el archivo `.env` en el directorio `ejercicio` contenga tu API key de OpenAI:

```env
OPENAI_API_KEY=tu_api_key_aqui
```

## ğŸ’» Uso

### Iniciar el chatbot

Desde el directorio `ejercicio`, ejecuta:

```bash
python main.py
```

### Comandos disponibles

Una vez iniciado el chatbot, puedes usar los siguientes comandos:

- **Hacer una pregunta**: Simplemente escribe tu pregunta y presiona Enter
- **/salir** o **quit**: Terminar la conversaciÃ³n
- **/reiniciar**: Reiniciar el historial de conversaciÃ³n
- **/historial**: Ver el historial completo de la conversaciÃ³n

### Ejemplos de preguntas

```
ğŸ‘¤ TÃº: Â¿CuÃ¡l es la misiÃ³n de la empresa?
ğŸ‘¤ TÃº: Â¿QuÃ© servicios ofrece TechInnovate Solutions?
ğŸ‘¤ TÃº: Â¿CuÃ¡l es la polÃ­tica de vacaciones?
ğŸ‘¤ TÃº: Â¿QuÃ© beneficios tienen los empleados?
ğŸ‘¤ TÃº: HÃ¡blame sobre el horario de trabajo
ğŸ‘¤ TÃº: Â¿QuÃ© certificaciones tiene la empresa?
```

## ğŸ§  Funcionamiento del Sistema RAG

1. **Carga de documentos**: Los archivos markdown se cargan desde el directorio `documents/`
2. **Chunking**: Los documentos se dividen en fragmentos mÃ¡s pequeÃ±os (chunks) de 1000 caracteres con 200 de overlap
3. **Embeddings**: Cada chunk se convierte en un vector usando `text-embedding-3-small`
4. **Vector Store**: Los vectores se almacenan en memoria usando `InMemoryVectorStore`
5. **Retrieval**: Cuando el usuario hace una pregunta:
   - La pregunta se convierte en un vector
   - Se buscan los 4 chunks mÃ¡s similares
   - Se concatenan para formar el contexto
6. **GeneraciÃ³n**: El LLM genera una respuesta usando:
   - El contexto recuperado
   - El historial de conversaciÃ³n
   - Un prompt del sistema que define el comportamiento

## ğŸ¯ Componentes Principales

### RAGSystem (`core/rag_system.py`)

Gestiona:
- ConfiguraciÃ³n de embeddings
- Carga y procesamiento de documentos
- CreaciÃ³n del vector store
- RecuperaciÃ³n de informaciÃ³n relevante

### Chatbot (`core/chatbot.py`)

Gestiona:
- IntegraciÃ³n con el sistema RAG
- InteracciÃ³n con el modelo de OpenAI
- Mantenimiento del historial conversacional
- GeneraciÃ³n de respuestas contextualizadas

### Main (`main.py`)

Gestiona:
- Interfaz de lÃ­nea de comandos
- Bucle de conversaciÃ³n
- Comandos del usuario
- Manejo de errores

## ğŸ“Š Modelos Disponibles

El chatbot puede usar cualquiera de estos modelos de OpenAI (configurable en `main.py`):

- **gpt-4o**: Modelo mÃ¡s capaz y reciente (por defecto)
- **gpt-4.1**: VersiÃ³n anterior de GPT-4
- **gpt-4o-mini**: VersiÃ³n mÃ¡s ligera y econÃ³mica

Para cambiar el modelo, edita la lÃ­nea en `main.py`:

```python
chatbot = Chatbot(rag_system=rag_system, api_key=api_key, model="gpt-4o")
```

## ğŸ“š Documentos de la Empresa

El sistema incluye dos documentos markdown ficticios:

1. **documento1.md**: InformaciÃ³n general sobre TechInnovate Solutions
   - Historia de la empresa
   - MisiÃ³n, visiÃ³n y valores
   - Servicios principales
   - Equipo y cultura
   - Clientes y certificaciones

2. **documento2.md**: PolÃ­ticas y procedimientos internos
   - PolÃ­ticas de RRHH
   - Horarios de trabajo
   - Beneficios sociales
   - CÃ³digo de conducta
   - Procedimientos operativos
   - EvaluaciÃ³n de desempeÃ±o

## ğŸ” PersonalizaciÃ³n

### AÃ±adir mÃ¡s documentos

Simplemente aÃ±ade archivos `.md` al directorio `documents/`. El sistema los procesarÃ¡ automÃ¡ticamente.

### Ajustar el tamaÃ±o de chunks

En `rag_system.py`, modifica los parÃ¡metros:

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # TamaÃ±o del chunk
    chunk_overlap=200,    # Overlap entre chunks
)
```

### Cambiar el nÃºmero de documentos recuperados

En `chatbot.py`, modifica el parÃ¡metro `k`:

```python
context = self.rag_system.get_context_for_query(user_message, k=4)
```

### Ajustar la temperatura del modelo

En `chatbot.py`, modifica:

```python
self.llm = ChatOpenAI(
    model=self.model,
    api_key=self.api_key,
    temperature=0.7  # MÃ¡s bajo = mÃ¡s determinista, mÃ¡s alto = mÃ¡s creativo
)
```

## âš ï¸ Limitaciones

- El chatbot solo puede responder basÃ¡ndose en la informaciÃ³n de los documentos
- La calidad de las respuestas depende de la calidad de los documentos
- Los documentos se cargan en memoria (no escalable para miles de documentos)
- Requiere conexiÃ³n a internet para llamadas a la API de OpenAI
- Tiene un costo asociado por uso de la API de OpenAI

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se encontrÃ³ la API key de OpenAI"
- Verifica que el archivo `.env` existe y contiene `OPENAI_API_KEY`
- AsegÃºrate de que la API key es vÃ¡lida

### Error: "Import could not be resolved"
- Ejecuta `pip install -r requirements.txt`
- Verifica que estÃ¡s usando Python 3.8 o superior

### El chatbot no responde correctamente
- Verifica que los documentos markdown estÃ¡n en `documents/`
- Comprueba que el modelo de OpenAI estÃ¡ disponible
- Revisa tu cuota de API de OpenAI

## ğŸ“ Licencia

Este es un proyecto educativo para fines de aprendizaje.

## ğŸ‘¥ Autor

Desarrollado como parte del Bootcamp de IA - Sprint 4

## ğŸ™ Agradecimientos

- LangChain por el framework RAG
- OpenAI por los modelos de embeddings y LLM
- La comunidad de Python por las excelentes librerÃ­as
